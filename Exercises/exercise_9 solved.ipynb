{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** In most sessions you will be solving exercises posed in a Jupyter notebook that looks like this one. Because you are cloning a Github repository that only we can push to, you should **NEVER EDIT** any of the files you pull from Github. Instead, what you should do, is either make a new notebook and write your solutions in there, or **make a copy of this notebook and save it somewhere else** on your computer, not inside the `sds` folder that you cloned, so you can write your answers in there. If you edit the notebook you pulled from Github, those edits (possible your solutions to the exercises) may be overwritten and lost the next time you pull from Github. This is important, so don't hesitate to ask if it is unclear.\n",
    "\n",
    "You should run `pip install scraping_class`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scraping_class\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import lxml\n",
    "connector = scraping_class.Connector(\"log.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T15:30:03.634114Z",
     "start_time": "2017-08-23T15:30:03.629294Z"
    }
   },
   "source": [
    "# Exercise Set 9: Parsing and Information Extraction\n",
    "\n",
    "*Morning, August 17, 2018*\n",
    "\n",
    "In this Exercise Set we shall develop our webscraping skills even further by practicing **parsing** and navigating html trees using BeautifoulSoup and furthermore train extracting information from raw text with no html tags to help, using regular expressions. \n",
    "\n",
    "But just as importantly you will get a chance to think about **data quality issues** and how to ensure reliability when curating your own webdata. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Section 9.1: Logging and data quality\n",
    "\n",
    "> **Ex. 9.1.1:** *Why is is it important to log processes in your data collection?*\n",
    "\n",
    "** answer goes here** \n",
    "\n",
    "\n",
    "\n",
    "> **Ex. 9.1.2:**\n",
    "\n",
    "*How does logging help with both ensuring and documenting the quality of your data?*\n",
    "** answer goes here** \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Section 9.2: Parsing a Table from HTML using BeautifulSoup.\n",
    "\n",
    "Yesterday I showed you a neat little prepackaged function in pandas that did all the work. However today we should learn the mechanics of it. *(It is not just for educational purposes, sometimes the package will not do exactly as you want.)*\n",
    "\n",
    "We hit the Basketball stats page from yesterday again: https://www.basketball-reference.com/leagues/NBA_2018.html.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 9.2.1:** Here we practice simply locating the table node of interest using the `find` method build into BeautifoulSoup. But first we have to fetch the HTML using the `requests` module. Parse the tree using `BeautifulSoup`. And then use the **>Inspector<** tool (* right click on the table < press inspect element *) in your browser to see how to locate the Eastern Conference table node - i.e. the *tag* name of the node, and maybe some defining *attributes*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "response, call_id = connector.get('https://www.basketball-reference.com/leagues/NBA_2018.html', 'looking-around')\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "soup_tables = soup.findAll('table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<table class=\"suppress_all sortable stats_table\" data-cols-to-freeze=\"1\" id=\"confs_standings_E\"><caption>Conference Standings Table</caption>\n",
       "<colgroup><col/><col/><col/><col/><col/><col/><col/><col/></colgroup>\n",
       "<thead>\n",
       "<tr>\n",
       "<th aria-label=\"Eastern Conference\" class=\"poptip sort_default_asc left\" data-stat=\"team_name\" scope=\"col\">Eastern Conference</th>\n",
       "<th aria-label=\"Wins\" class=\"poptip right\" data-stat=\"wins\" data-tip=\"Wins\" scope=\"col\">W</th>\n",
       "<th aria-label=\"Losses\" class=\"poptip right\" data-stat=\"losses\" data-tip=\"Losses\" scope=\"col\">L</th>\n",
       "<th aria-label=\"Win-Loss Percentage\" class=\"poptip right\" data-stat=\"win_loss_pct\" data-tip=\"Win-Loss Percentage\" scope=\"col\">W/L%</th>\n",
       "<th aria-label=\"Games Behind\" class=\"poptip sort_default_asc right\" data-stat=\"gb\" data-tip=\"Games Behind\" scope=\"col\">GB</th>\n",
       "<th aria-label=\"Points Per Game\" class=\"poptip right\" data-stat=\"pts_per_g\" data-tip=\"Points Per Game\" scope=\"col\">PS/G</th>\n",
       "<th aria-label=\"Opponent Points Per Game\" class=\"poptip right\" data-stat=\"opp_pts_per_g\" data-tip=\"Opponent Points Per Game\" scope=\"col\">PA/G</th>\n",
       "<th aria-label=\"Simple Rating System\" class=\"poptip right\" data-stat=\"srs\" data-tip=\"Simple Rating System; a team rating that takes into account average point differential and strength of schedule. The rating is denominated in points above/below average, where zero is average.\" scope=\"col\">SRS</th>\n",
       "</tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr class=\"full_table\"><th class=\"left\" data-stat=\"team_name\" scope=\"row\"><a href=\"/teams/TOR/2018.html\">Toronto Raptors</a>* <span class=\"seed\">(1) </span></th><td class=\"right\" data-stat=\"wins\">59</td><td class=\"right\" data-stat=\"losses\">23</td><td class=\"right\" data-stat=\"win_loss_pct\">.720</td><td class=\"right\" data-stat=\"gb\">—</td><td class=\"right\" data-stat=\"pts_per_g\">111.7</td><td class=\"right\" data-stat=\"opp_pts_per_g\">103.9</td><td class=\"right\" data-stat=\"srs\">7.29</td></tr>\n",
       "<tr class=\"full_table\"><th class=\"left\" data-stat=\"team_name\" scope=\"row\"><a href=\"/teams/BOS/2018.html\">Boston Celtics</a>* <span class=\"seed\">(2) </span></th><td class=\"right\" data-stat=\"wins\">55</td><td class=\"right\" data-stat=\"losses\">27</td><td class=\"right\" data-stat=\"win_loss_pct\">.671</td><td class=\"right\" data-stat=\"gb\">4.0</td><td class=\"right\" data-stat=\"pts_per_g\">104.0</td><td class=\"right\" data-stat=\"opp_pts_per_g\">100.4</td><td class=\"right\" data-stat=\"srs\">3.23</td></tr>\n",
       "<tr class=\"full_table\"><th class=\"left\" data-stat=\"team_name\" scope=\"row\"><a href=\"/teams/PHI/2018.html\">Philadelphia 76ers</a>* <span class=\"seed\">(3) </span></th><td class=\"right\" data-stat=\"wins\">52</td><td class=\"right\" data-stat=\"losses\">30</td><td class=\"right\" data-stat=\"win_loss_pct\">.634</td><td class=\"right\" data-stat=\"gb\">7.0</td><td class=\"right\" data-stat=\"pts_per_g\">109.8</td><td class=\"right\" data-stat=\"opp_pts_per_g\">105.3</td><td class=\"right\" data-stat=\"srs\">4.30</td></tr>\n",
       "<tr class=\"full_table\"><th class=\"left\" data-stat=\"team_name\" scope=\"row\"><a href=\"/teams/CLE/2018.html\">Cleveland Cavaliers</a>* <span class=\"seed\">(4) </span></th><td class=\"right\" data-stat=\"wins\">50</td><td class=\"right\" data-stat=\"losses\">32</td><td class=\"right\" data-stat=\"win_loss_pct\">.610</td><td class=\"right\" data-stat=\"gb\">9.0</td><td class=\"right\" data-stat=\"pts_per_g\">110.9</td><td class=\"right\" data-stat=\"opp_pts_per_g\">109.9</td><td class=\"right\" data-stat=\"srs\">0.59</td></tr>\n",
       "<tr class=\"full_table\"><th class=\"left\" data-stat=\"team_name\" scope=\"row\"><a href=\"/teams/IND/2018.html\">Indiana Pacers</a>* <span class=\"seed\">(5) </span></th><td class=\"right\" data-stat=\"wins\">48</td><td class=\"right\" data-stat=\"losses\">34</td><td class=\"right\" data-stat=\"win_loss_pct\">.585</td><td class=\"right\" data-stat=\"gb\">11.0</td><td class=\"right\" data-stat=\"pts_per_g\">105.6</td><td class=\"right\" data-stat=\"opp_pts_per_g\">104.2</td><td class=\"right\" data-stat=\"srs\">1.18</td></tr>\n",
       "<tr class=\"full_table\"><th class=\"left\" data-stat=\"team_name\" scope=\"row\"><a href=\"/teams/MIA/2018.html\">Miami Heat</a>* <span class=\"seed\">(6) </span></th><td class=\"right\" data-stat=\"wins\">44</td><td class=\"right\" data-stat=\"losses\">38</td><td class=\"right\" data-stat=\"win_loss_pct\">.537</td><td class=\"right\" data-stat=\"gb\">15.0</td><td class=\"right\" data-stat=\"pts_per_g\">103.4</td><td class=\"right\" data-stat=\"opp_pts_per_g\">102.9</td><td class=\"right\" data-stat=\"srs\">0.15</td></tr>\n",
       "<tr class=\"full_table\"><th class=\"left\" data-stat=\"team_name\" scope=\"row\"><a href=\"/teams/MIL/2018.html\">Milwaukee Bucks</a>* <span class=\"seed\">(7) </span></th><td class=\"right\" data-stat=\"wins\">44</td><td class=\"right\" data-stat=\"losses\">38</td><td class=\"right\" data-stat=\"win_loss_pct\">.537</td><td class=\"right\" data-stat=\"gb\">15.0</td><td class=\"right\" data-stat=\"pts_per_g\">106.5</td><td class=\"right\" data-stat=\"opp_pts_per_g\">106.8</td><td class=\"right\" data-stat=\"srs\">-0.45</td></tr>\n",
       "<tr class=\"full_table\"><th class=\"left\" data-stat=\"team_name\" scope=\"row\"><a href=\"/teams/WAS/2018.html\">Washington Wizards</a>* <span class=\"seed\">(8) </span></th><td class=\"right\" data-stat=\"wins\">43</td><td class=\"right\" data-stat=\"losses\">39</td><td class=\"right\" data-stat=\"win_loss_pct\">.524</td><td class=\"right\" data-stat=\"gb\">16.0</td><td class=\"right\" data-stat=\"pts_per_g\">106.6</td><td class=\"right\" data-stat=\"opp_pts_per_g\">106.0</td><td class=\"right\" data-stat=\"srs\">0.53</td></tr>\n",
       "<tr class=\"full_table\"><th class=\"left\" data-stat=\"team_name\" scope=\"row\"><a href=\"/teams/DET/2018.html\">Detroit Pistons</a> <span class=\"seed\">(9) </span></th><td class=\"right\" data-stat=\"wins\">39</td><td class=\"right\" data-stat=\"losses\">43</td><td class=\"right\" data-stat=\"win_loss_pct\">.476</td><td class=\"right\" data-stat=\"gb\">20.0</td><td class=\"right\" data-stat=\"pts_per_g\">103.8</td><td class=\"right\" data-stat=\"opp_pts_per_g\">103.9</td><td class=\"right\" data-stat=\"srs\">-0.26</td></tr>\n",
       "<tr class=\"full_table\"><th class=\"left\" data-stat=\"team_name\" scope=\"row\"><a href=\"/teams/CHO/2018.html\">Charlotte Hornets</a> <span class=\"seed\">(10) </span></th><td class=\"right\" data-stat=\"wins\">36</td><td class=\"right\" data-stat=\"losses\">46</td><td class=\"right\" data-stat=\"win_loss_pct\">.439</td><td class=\"right\" data-stat=\"gb\">23.0</td><td class=\"right\" data-stat=\"pts_per_g\">108.2</td><td class=\"right\" data-stat=\"opp_pts_per_g\">108.0</td><td class=\"right\" data-stat=\"srs\">0.07</td></tr>\n",
       "<tr class=\"full_table\"><th class=\"left\" data-stat=\"team_name\" scope=\"row\"><a href=\"/teams/NYK/2018.html\">New York Knicks</a> <span class=\"seed\">(11) </span></th><td class=\"right\" data-stat=\"wins\">29</td><td class=\"right\" data-stat=\"losses\">53</td><td class=\"right\" data-stat=\"win_loss_pct\">.354</td><td class=\"right\" data-stat=\"gb\">30.0</td><td class=\"right\" data-stat=\"pts_per_g\">104.5</td><td class=\"right\" data-stat=\"opp_pts_per_g\">108.0</td><td class=\"right\" data-stat=\"srs\">-3.53</td></tr>\n",
       "<tr class=\"full_table\"><th class=\"left\" data-stat=\"team_name\" scope=\"row\"><a href=\"/teams/BRK/2018.html\">Brooklyn Nets</a> <span class=\"seed\">(12) </span></th><td class=\"right\" data-stat=\"wins\">28</td><td class=\"right\" data-stat=\"losses\">54</td><td class=\"right\" data-stat=\"win_loss_pct\">.341</td><td class=\"right\" data-stat=\"gb\">31.0</td><td class=\"right\" data-stat=\"pts_per_g\">106.6</td><td class=\"right\" data-stat=\"opp_pts_per_g\">110.3</td><td class=\"right\" data-stat=\"srs\">-3.67</td></tr>\n",
       "<tr class=\"full_table\"><th class=\"left\" data-stat=\"team_name\" scope=\"row\"><a href=\"/teams/CHI/2018.html\">Chicago Bulls</a> <span class=\"seed\">(13) </span></th><td class=\"right\" data-stat=\"wins\">27</td><td class=\"right\" data-stat=\"losses\">55</td><td class=\"right\" data-stat=\"win_loss_pct\">.329</td><td class=\"right\" data-stat=\"gb\">32.0</td><td class=\"right\" data-stat=\"pts_per_g\">102.9</td><td class=\"right\" data-stat=\"opp_pts_per_g\">110.0</td><td class=\"right\" data-stat=\"srs\">-6.84</td></tr>\n",
       "<tr class=\"full_table\"><th class=\"left\" data-stat=\"team_name\" scope=\"row\"><a href=\"/teams/ORL/2018.html\">Orlando Magic</a> <span class=\"seed\">(14) </span></th><td class=\"right\" data-stat=\"wins\">25</td><td class=\"right\" data-stat=\"losses\">57</td><td class=\"right\" data-stat=\"win_loss_pct\">.305</td><td class=\"right\" data-stat=\"gb\">34.0</td><td class=\"right\" data-stat=\"pts_per_g\">103.4</td><td class=\"right\" data-stat=\"opp_pts_per_g\">108.2</td><td class=\"right\" data-stat=\"srs\">-4.92</td></tr>\n",
       "<tr class=\"full_table\"><th class=\"left\" data-stat=\"team_name\" scope=\"row\"><a href=\"/teams/ATL/2018.html\">Atlanta Hawks</a> <span class=\"seed\">(15) </span></th><td class=\"right\" data-stat=\"wins\">24</td><td class=\"right\" data-stat=\"losses\">58</td><td class=\"right\" data-stat=\"win_loss_pct\">.293</td><td class=\"right\" data-stat=\"gb\">35.0</td><td class=\"right\" data-stat=\"pts_per_g\">103.4</td><td class=\"right\" data-stat=\"opp_pts_per_g\">108.8</td><td class=\"right\" data-stat=\"srs\">-5.30</td></tr>\n",
       "</tbody></table>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup_tables[0] #This is the html for the first table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have located the table should now build a function that starts at a \"table node\" and parses the information, and outputs a pandas DataFrame. \n",
    "\n",
    "Inspect the element either within the notebook or through the **>Inspector<** tool and start to see how a table is written in html. Which tag names can be used to locate rows? How will you iterate through columns. Were is the header located?\n",
    "\n",
    "> **Ex. 9.2.2:** First you parse the header which can be found in the canonical tag name: thead. \n",
    "Next you use the `find_all` method to search for the tag, and iterate through each of the elements extracting the text, using the `.text` method builtin to the the node object. Store the header values in a list container. \n",
    "\n",
    "> **Ex. 9.2.3:** Next you locate the rows, using the canonical tag name: tbody. And from here you search for all rows tags. Fiugre out the tag name yourself, inspecting the tbody node in python or using the **Inspector**. \n",
    "\n",
    "> **Ex. 9.2.4:** Next run through all the rows and extract each value, similar to how you extracted the header. However here is a slight variation: Since each value node can have a different tag depending on whether it is a digit or a string, you should use the `.children` method instead of the `.find_all` - (or write compile a regex that matches both the td tag and the th tag.) \n",
    ">Once the value nodes of each row has been located using the `.children` method you should extract the value. Store the extracted rows as a list of lists: ```[[val1,val2,...valk],...]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tables(html):\n",
    "    soup_tables = BeautifulSoup(html, 'lxml').findAll('table') #findall tables in the html\n",
    "    dataframes = {}\n",
    "    for table in soup_tables: #iterate through tables\n",
    "        ths = table.find('thead').findAll('th') #find all tableheaders, i.e. the <th> html-tag\n",
    "        column_headers = [th.text for th in ths] #extract the text from all the <th>-tags\n",
    "        \n",
    "        data = []\n",
    "        for tr in table.find('tbody').findAll('tr'): #iterate through tablerows, i.e. the <tr> html-tag\n",
    "            data.append([td.text for td in tr.children]) #extract the text from children of a <tr>-tag\n",
    "            \n",
    "        dataframes[table['id']] = pd.DataFrame(data, columns = column_headers) #save table to the dictionary\n",
    "\n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'confs_standings_E':            Eastern Conference   W   L  W/L%    GB   PS/G   PA/G    SRS\n",
       " 0       Toronto Raptors* (1)   59  23  .720     —  111.7  103.9   7.29\n",
       " 1        Boston Celtics* (2)   55  27  .671   4.0  104.0  100.4   3.23\n",
       " 2    Philadelphia 76ers* (3)   52  30  .634   7.0  109.8  105.3   4.30\n",
       " 3   Cleveland Cavaliers* (4)   50  32  .610   9.0  110.9  109.9   0.59\n",
       " 4        Indiana Pacers* (5)   48  34  .585  11.0  105.6  104.2   1.18\n",
       " 5            Miami Heat* (6)   44  38  .537  15.0  103.4  102.9   0.15\n",
       " 6       Milwaukee Bucks* (7)   44  38  .537  15.0  106.5  106.8  -0.45\n",
       " 7    Washington Wizards* (8)   43  39  .524  16.0  106.6  106.0   0.53\n",
       " 8        Detroit Pistons (9)   39  43  .476  20.0  103.8  103.9  -0.26\n",
       " 9     Charlotte Hornets (10)   36  46  .439  23.0  108.2  108.0   0.07\n",
       " 10      New York Knicks (11)   29  53  .354  30.0  104.5  108.0  -3.53\n",
       " 11        Brooklyn Nets (12)   28  54  .341  31.0  106.6  110.3  -3.67\n",
       " 12        Chicago Bulls (13)   27  55  .329  32.0  102.9  110.0  -6.84\n",
       " 13        Orlando Magic (14)   25  57  .305  34.0  103.4  108.2  -4.92\n",
       " 14        Atlanta Hawks (15)   24  58  .293  35.0  103.4  108.8  -5.30,\n",
       " 'confs_standings_W':               Western Conference   W   L  W/L%    GB   PS/G   PA/G    SRS\n",
       " 0          Houston Rockets* (1)   65  17  .793     —  112.4  103.9   8.21\n",
       " 1    Golden State Warriors* (2)   58  24  .707   7.0  113.5  107.5   5.79\n",
       " 2   Portland Trail Blazers* (3)   49  33  .598  16.0  105.6  103.0   2.60\n",
       " 3    Oklahoma City Thunder* (4)   48  34  .585  17.0  107.9  104.4   3.42\n",
       " 4                Utah Jazz* (5)   48  34  .585  17.0  104.1   99.8   4.47\n",
       " 5     New Orleans Pelicans* (6)   48  34  .585  17.0  111.7  110.4   1.48\n",
       " 6        San Antonio Spurs* (7)   47  35  .573  18.0  102.7   99.8   2.89\n",
       " 7   Minnesota Timberwolves* (8)   47  35  .573  18.0  109.5  107.3   2.35\n",
       " 8            Denver Nuggets (9)   46  36  .561  19.0  110.0  108.5   1.57\n",
       " 9     Los Angeles Clippers (10)   42  40  .512  23.0  109.0  109.0   0.15\n",
       " 10      Los Angeles Lakers (11)   35  47  .427  30.0  108.1  109.6  -1.44\n",
       " 11        Sacramento Kings (12)   27  55  .329  38.0   98.8  105.8  -6.60\n",
       " 12        Dallas Mavericks (13)   24  58  .293  41.0  102.3  105.4  -2.70\n",
       " 13       Memphis Grizzlies (14)   22  60  .268  43.0   99.3  105.5  -5.81\n",
       " 14            Phoenix Suns (15)   21  61  .256  44.0  103.9  113.3  -8.80,\n",
       " 'divs_standings_E':            Eastern Conference     W     L  W/L%    GB   PS/G   PA/G    SRS\n",
       " 0           Atlantic Division  None  None  None  None   None   None   None\n",
       " 1       Toronto Raptors* (1)     59    23  .720     —  111.7  103.9   7.29\n",
       " 2        Boston Celtics* (2)     55    27  .671   4.0  104.0  100.4   3.23\n",
       " 3    Philadelphia 76ers* (3)     52    30  .634   7.0  109.8  105.3   4.30\n",
       " 4       New York Knicks (11)     29    53  .354  30.0  104.5  108.0  -3.53\n",
       " 5         Brooklyn Nets (12)     28    54  .341  31.0  106.6  110.3  -3.67\n",
       " 6            Central Division  None  None  None  None   None   None   None\n",
       " 7   Cleveland Cavaliers* (4)     50    32  .610     —  110.9  109.9   0.59\n",
       " 8        Indiana Pacers* (5)     48    34  .585   2.0  105.6  104.2   1.18\n",
       " 9       Milwaukee Bucks* (7)     44    38  .537   6.0  106.5  106.8  -0.45\n",
       " 10       Detroit Pistons (9)     39    43  .476  11.0  103.8  103.9  -0.26\n",
       " 11        Chicago Bulls (13)     27    55  .329  23.0  102.9  110.0  -6.84\n",
       " 12         Southeast Division  None  None  None  None   None   None   None\n",
       " 13           Miami Heat* (6)     44    38  .537     —  103.4  102.9   0.15\n",
       " 14   Washington Wizards* (8)     43    39  .524   1.0  106.6  106.0   0.53\n",
       " 15    Charlotte Hornets (10)     36    46  .439   8.0  108.2  108.0   0.07\n",
       " 16        Orlando Magic (14)     25    57  .305  19.0  103.4  108.2  -4.92\n",
       " 17        Atlanta Hawks (15)     24    58  .293  20.0  103.4  108.8  -5.30,\n",
       " 'divs_standings_W':               Western Conference     W     L  W/L%    GB   PS/G   PA/G    SRS\n",
       " 0             Northwest Division  None  None  None  None   None   None   None\n",
       " 1   Portland Trail Blazers* (3)     49    33  .598     —  105.6  103.0   2.60\n",
       " 2    Oklahoma City Thunder* (4)     48    34  .585   1.0  107.9  104.4   3.42\n",
       " 3                Utah Jazz* (5)     48    34  .585   1.0  104.1   99.8   4.47\n",
       " 4   Minnesota Timberwolves* (8)     47    35  .573   2.0  109.5  107.3   2.35\n",
       " 5            Denver Nuggets (9)     46    36  .561   3.0  110.0  108.5   1.57\n",
       " 6               Pacific Division  None  None  None  None   None   None   None\n",
       " 7    Golden State Warriors* (2)     58    24  .707     —  113.5  107.5   5.79\n",
       " 8     Los Angeles Clippers (10)     42    40  .512  16.0  109.0  109.0   0.15\n",
       " 9       Los Angeles Lakers (11)     35    47  .427  23.0  108.1  109.6  -1.44\n",
       " 10        Sacramento Kings (12)     27    55  .329  31.0   98.8  105.8  -6.60\n",
       " 11            Phoenix Suns (15)     21    61  .256  37.0  103.9  113.3  -8.80\n",
       " 12            Southwest Division  None  None  None  None   None   None   None\n",
       " 13         Houston Rockets* (1)     65    17  .793     —  112.4  103.9   8.21\n",
       " 14    New Orleans Pelicans* (6)     48    34  .585  17.0  111.7  110.4   1.48\n",
       " 15       San Antonio Spurs* (7)     47    35  .573  18.0  102.7   99.8   2.89\n",
       " 16        Dallas Mavericks (13)     24    58  .293  41.0  102.3  105.4  -2.70\n",
       " 17       Memphis Grizzlies (14)     22    60  .268  43.0   99.3  105.5  -5.81}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NBA_dfs = get_tables(response.text)\n",
    "NBA_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex. 9.2.5** Convert the data you have collected into a pandas dataframe. _Bonus:_ convert the code you've written above into a function which scrapes the page and returns a dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[Answer 9.2.5]\n",
    "# see above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 9.2.6:** Now locate all tables from the page, using the `.find_all` method searching for the table tag name. Iterate through the table nodes and apply the function created for parsing html tables. Store each table in a dictionary using the table name as key. The name is found by accessing the id attribute of each table node, using dictionary-style syntax - i.e. `table_node['id']`.\n",
    "\n",
    "> **9.2.extra.:** Compare your results to the pandas implementation. pd.read_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 9.1.6]\n",
    "# see above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Section 9.3: Practicing Regular Expressions.\n",
    "This exercise is about developing your experience with designing your own regular expressions.\n",
    "\n",
    "Remember you can always consult the regular expression reference page [here](https://www.regular-expressions.info/refquick.html), if you need to remember or understand a specific symbol. \n",
    "\n",
    "You should practice using *\"define-inspect-refine-method\"* described in the lectures to systematically ***explore*** and ***refine*** your expressions, and save all the patterns tried. You can download the small module that I created to handle this in the following way: \n",
    "``` python\n",
    "import requests\n",
    "url = 'https://raw.githubusercontent.com/snorreralund/explore_regex/master/explore_regex.py'\n",
    "response = requests.get(url)\n",
    "with open('explore_regex.py','w') as f:\n",
    "    f.write(response.text)\n",
    "import explore_regex as e_re\n",
    "```\n",
    "\n",
    "Remember to start ***broad*** to gain many examples, and iteratively narrow and refine.\n",
    "\n",
    "We will use a sample of the trustpilot dataset that you practiced collecting yesterday.\n",
    "You can load it directly into python from the following link: https://raw.githubusercontent.com/snorreralund/scraping_seminar/master/english_review_sample.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 9.3.0:** Load the data used in the exercise using the `pd.read_csv` function. (Hint: path to file can be both a url or systempath). \n",
    "\n",
    ">Define a variable `sample_string = '\\n'.join(df.sample(2000).reviewBody)` as sample of all the reviews that you will practice on.  (Run it once in a while to get a new sample for potential differences).\n",
    "Imagine we were a company wanting to find the reviews where customers are concerned with the price of a service. They decide to write a regular expression to match all reviews where a currencies and an amount is mentioned. \n",
    "\n",
    "> **Ex. 9.3.1:** \n",
    "> Write an expression that matches both the dollar-sign ($) and dollar written literally, and the amount before or after a dollar-sign. Remember that the \"$\"-sign is a special character in regular expressions. Explore and refine using the explore_pattern function in the package I created called explore_regex. \n",
    "```python\n",
    "import explore_regex as e_re\n",
    "explore_regex = e_re.Explore_Regex(sample_string) # Initaizlie the Explore regex Class.\n",
    "explore_regex.explore_pattern(pattern) # Use the .explore_pattern method.\n",
    "```\n",
    "\n",
    "\n",
    "Start with exploring the context around digits (\"\\d\") in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "# download data\n",
    "path2data = 'https://raw.githubusercontent.com/snorreralund/scraping_seminar/master/english_review_sample.csv'\n",
    "df = pd.read_csv(path2data)\n",
    "# download module\n",
    "url = 'https://raw.githubusercontent.com/snorreralund/explore_regex/master/explore_regex.py'\n",
    "response = requests.get(url)\n",
    "# write script to your folder to create a locate module\n",
    "with open('explore_regex.py','w') as f:\n",
    "    f.write(response.text)\n",
    "# import local module\n",
    "import explore_regex as e_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "digit_re = re.compile('[0-9]+') \n",
    "df['hasNumber'] = df['reviewBody'].apply(lambda x: bool(digit_re.search(x)))\n",
    "sample_string = '\\n'.join(df[df['hasNumber']].sample(1000).reviewBody)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewBody</th>\n",
       "      <th>hasNumber</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lots of inventory, very fast and efficient. I ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I did not received the map I had ordered and p...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>After searching a number of stores here in my ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Website is not intuitive.  I don't like having...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Outstanding customer service, appreciated the ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          reviewBody  hasNumber\n",
       "0  Lots of inventory, very fast and efficient. I ...      False\n",
       "1  I did not received the map I had ordered and p...      False\n",
       "2  After searching a number of stores here in my ...      False\n",
       "3  Website is not intuitive.  I don't like having...      False\n",
       "4  Outstanding customer service, appreciated the ...      False"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['reviewBody', 'hasNumber']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_regex = e_re.ExploreRegex(sample_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Pattern: ((\\d*\\.)*\\d+\\s*(dollar|\\$))|((dollar|\\$)\\s*(\\d*\\.)*\\d+)\t Matched 247 patterns -----\n",
      "Match: $2\tContext:ckage for $2 shipping.\n",
      "Match: $79\tContext:sed to be $79 USD and e\n",
      "Match: $20\tContext: food for $20 less a ba\n",
      "Match: $10\tContext:reased to $10.  Imagine\n",
      "Match: 10dollar\tContext: belts at 10dollarmall for p\n",
      "Match: $1000\tContext: price of $1000.  Your pr\n",
      "Match: $25.00\tContext:ought the $25.00 gift card\n",
      "Match: 10$\tContext:s I get a 10$ off coupo\n",
      "Match: $100\tContext: It was a $100 fix, and \n",
      "Match: $99\tContext:wn to the $99 service s\n",
      "Match: $200\tContext:serve for $200.  Returne\n",
      "Match: $164.00\tContext: I saved  $164.00 and did n\n",
      "Match: $200\tContext:ot so the $200.- 3 year \n",
      "Match: $1\tContext:dditional $1,000 when \n",
      "Match: $277\tContext: $65 on a $277 order. Th\n",
      "Match: $10\tContext:nce—apply $10 off now, \n",
      "Match: $100\tContext:I ordered $100 plus in c\n",
      "Match: $100\tContext:nly worth $100 on non-di\n",
      "Match: $900\tContext:ng at the $900+ alternat\n",
      "Match: $149\tContext:g up with $149 website I\n",
      "Match: $5\tContext:han spend $5 for a sec\n",
      "Match: $26\tContext:l cost of $26 for a cal\n",
      "Match: $25\tContext:cter over $25. The serv\n",
      "Match: $6\tContext:avings of $6,000.  Ver\n",
      "Match: $20\tContext:they were $20 cheaper t\n",
      "Match: $125\tContext:irs under $125\n",
      "I brought\n",
      "Match: $.99\tContext:etimes  a $.99 or $1.99 \n",
      "Match: $270\tContext: price as $270 and I bou\n",
      "Match: $100\tContext:ange was >$100.  I put i\n",
      "Match: $50\tContext:more then $50 you can c\n"
     ]
    }
   ],
   "source": [
    "#price_pattern matches prices including decimals. The decimals are the '(\\d*\\.)*' part\n",
    "price_pattern = r'((\\d*\\.)*\\d+\\s*(dollar|\\$))|((dollar|\\$)\\s*(\\d*\\.)*\\d+)'\n",
    "explore_regex.explore_pattern(price_pattern, n_samples = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex.9.3.3** Use the .report() method. e_re.report(), and print the all patterns in the development process using the .pattern method - i.e. e_re.patterns \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 9.3.4** \n",
    "Finally write a function that takes in a string and outputs if there is a match. Use the .match function to see if there is a match (hint if does not return a NoneType object - `re.match(pattern,string)!=None`).\n",
    "\n",
    "> Define a column 'mention_currency' in the dataframe, by applying the above function to the text column of the dataframe. \n",
    "*** You should have approximately 310 reviews that matches. - but less is also alright***\n",
    "\n",
    "> **Ex. 9.3.5** Explore the relation between reviews mentioning prices and the average rating. \n",
    "\n",
    "> **Ex. 9.3.extra** Define a function that outputs the amount mentioned in the review (if more than one the largest), define a new column by applying it to the data, and explore whether reviews mentioning higher prices are worse than others by plotting the amount versus the rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>__domain__</th>\n",
       "      <th>address_@type</th>\n",
       "      <th>address_addressCountry</th>\n",
       "      <th>address_addressLocality</th>\n",
       "      <th>address_postalCode</th>\n",
       "      <th>address_streetAddress</th>\n",
       "      <th>author_@type</th>\n",
       "      <th>datePublished</th>\n",
       "      <th>email</th>\n",
       "      <th>...</th>\n",
       "      <th>itemReviewed_name</th>\n",
       "      <th>meta_@type</th>\n",
       "      <th>name</th>\n",
       "      <th>reviewBody</th>\n",
       "      <th>reviewRating_@type</th>\n",
       "      <th>reviewRating_ratingValue</th>\n",
       "      <th>telephone</th>\n",
       "      <th>categories</th>\n",
       "      <th>hasNumber</th>\n",
       "      <th>mentionCurrency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>159770</td>\n",
       "      <td>https://trustpilot.com/review/www.exmed.net</td>\n",
       "      <td>PostalAddress</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fenton</td>\n",
       "      <td>63026</td>\n",
       "      <td>218 Seebold Spur</td>\n",
       "      <td>Person</td>\n",
       "      <td>2017-07-29T20:27:03Z</td>\n",
       "      <td>sales@exmed.net</td>\n",
       "      <td>...</td>\n",
       "      <td>Express Medical Supply</td>\n",
       "      <td>LocalBusiness</td>\n",
       "      <td>Express Medical Supply</td>\n",
       "      <td>Lots of inventory, very fast and efficient. I ...</td>\n",
       "      <td>Rating</td>\n",
       "      <td>5</td>\n",
       "      <td>(800) 633-2139</td>\n",
       "      <td>/health_wellbeing</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>168724</td>\n",
       "      <td>https://trustpilot.com/review/mapscompany.com</td>\n",
       "      <td>PostalAddress</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Petit-Rocher, NB</td>\n",
       "      <td>E8J 1E4</td>\n",
       "      <td>713 rue de la Mer</td>\n",
       "      <td>Person</td>\n",
       "      <td>2017-08-11T20:09:48Z</td>\n",
       "      <td>contact@mapscompany.com</td>\n",
       "      <td>...</td>\n",
       "      <td>MapsCompany</td>\n",
       "      <td>LocalBusiness</td>\n",
       "      <td>MapsCompany</td>\n",
       "      <td>I did not received the map I had ordered and p...</td>\n",
       "      <td>Rating</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/travel_holidays</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>96443</td>\n",
       "      <td>https://trustpilot.com/review/www.thriftbooks.com</td>\n",
       "      <td>PostalAddress</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tukwila</td>\n",
       "      <td>98188</td>\n",
       "      <td>18300 Cascade Ave S, Ste 150</td>\n",
       "      <td>Person</td>\n",
       "      <td>2015-03-19T22:59:22Z</td>\n",
       "      <td>reviews@thriftbooks.com</td>\n",
       "      <td>...</td>\n",
       "      <td>Thrift Books</td>\n",
       "      <td>LocalBusiness</td>\n",
       "      <td>Thrift Books</td>\n",
       "      <td>After searching a number of stores here in my ...</td>\n",
       "      <td>Rating</td>\n",
       "      <td>5</td>\n",
       "      <td>253-275-2251</td>\n",
       "      <td>/entertainment</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>173433</td>\n",
       "      <td>https://trustpilot.com/review/fabletics.com</td>\n",
       "      <td>PostalAddress</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Person</td>\n",
       "      <td>2017-04-30T19:47:39Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Fabletics</td>\n",
       "      <td>LocalBusiness</td>\n",
       "      <td>Fabletics</td>\n",
       "      <td>Website is not intuitive.  I don't like having...</td>\n",
       "      <td>Rating</td>\n",
       "      <td>2</td>\n",
       "      <td>855-202-3570</td>\n",
       "      <td>/clothes_fashion</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>138968</td>\n",
       "      <td>https://trustpilot.com/review/www.enterprise.com</td>\n",
       "      <td>PostalAddress</td>\n",
       "      <td>US</td>\n",
       "      <td>St Louis</td>\n",
       "      <td>63105</td>\n",
       "      <td>600 Corporate Park Dr</td>\n",
       "      <td>Person</td>\n",
       "      <td>2018-05-26T20:43:41Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Enterprise</td>\n",
       "      <td>LocalBusiness</td>\n",
       "      <td>Enterprise</td>\n",
       "      <td>Outstanding customer service, appreciated the ...</td>\n",
       "      <td>Rating</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/transportation</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                         __domain__  \\\n",
       "0      159770        https://trustpilot.com/review/www.exmed.net   \n",
       "1      168724      https://trustpilot.com/review/mapscompany.com   \n",
       "2       96443  https://trustpilot.com/review/www.thriftbooks.com   \n",
       "3      173433        https://trustpilot.com/review/fabletics.com   \n",
       "4      138968   https://trustpilot.com/review/www.enterprise.com   \n",
       "\n",
       "   address_@type address_addressCountry address_addressLocality  \\\n",
       "0  PostalAddress                    NaN                  Fenton   \n",
       "1  PostalAddress                    NaN        Petit-Rocher, NB   \n",
       "2  PostalAddress                    NaN                 Tukwila   \n",
       "3  PostalAddress                    NaN                     NaN   \n",
       "4  PostalAddress                     US                St Louis   \n",
       "\n",
       "  address_postalCode         address_streetAddress author_@type  \\\n",
       "0              63026              218 Seebold Spur       Person   \n",
       "1            E8J 1E4             713 rue de la Mer       Person   \n",
       "2              98188  18300 Cascade Ave S, Ste 150       Person   \n",
       "3                NaN                           NaN       Person   \n",
       "4              63105         600 Corporate Park Dr       Person   \n",
       "\n",
       "          datePublished                    email  ...       itemReviewed_name  \\\n",
       "0  2017-07-29T20:27:03Z          sales@exmed.net  ...  Express Medical Supply   \n",
       "1  2017-08-11T20:09:48Z  contact@mapscompany.com  ...             MapsCompany   \n",
       "2  2015-03-19T22:59:22Z  reviews@thriftbooks.com  ...            Thrift Books   \n",
       "3  2017-04-30T19:47:39Z                      NaN  ...               Fabletics   \n",
       "4  2018-05-26T20:43:41Z                      NaN  ...              Enterprise   \n",
       "\n",
       "      meta_@type                    name  \\\n",
       "0  LocalBusiness  Express Medical Supply   \n",
       "1  LocalBusiness             MapsCompany   \n",
       "2  LocalBusiness            Thrift Books   \n",
       "3  LocalBusiness               Fabletics   \n",
       "4  LocalBusiness              Enterprise   \n",
       "\n",
       "                                          reviewBody reviewRating_@type  \\\n",
       "0  Lots of inventory, very fast and efficient. I ...             Rating   \n",
       "1  I did not received the map I had ordered and p...             Rating   \n",
       "2  After searching a number of stores here in my ...             Rating   \n",
       "3  Website is not intuitive.  I don't like having...             Rating   \n",
       "4  Outstanding customer service, appreciated the ...             Rating   \n",
       "\n",
       "  reviewRating_ratingValue       telephone         categories  hasNumber  \\\n",
       "0                        5  (800) 633-2139  /health_wellbeing      False   \n",
       "1                        3             NaN   /travel_holidays      False   \n",
       "2                        5    253-275-2251     /entertainment      False   \n",
       "3                        2    855-202-3570   /clothes_fashion      False   \n",
       "4                        5             NaN    /transportation      False   \n",
       "\n",
       "  mentionCurrency  \n",
       "0           False  \n",
       "1           False  \n",
       "2           False  \n",
       "3           False  \n",
       "4           False  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#9.3.4\n",
    "def mention_currency(span):\n",
    "    m = re.compile(r'((\\d*\\.)*\\d+\\s*(dollar|\\$))|((dollar|\\$)\\s*(\\d*\\.)*\\d+)').search(span)\n",
    "    return bool(m)\n",
    "\n",
    "df['mentionCurrency'] = df['reviewBody'].apply(mention_currency)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mentionCurrency\n",
       "False    4.502371\n",
       "True     3.046667\n",
       "Name: reviewRating_ratingValue, dtype: float64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#9.3.5\n",
    "df.groupby('mentionCurrency').mean()['reviewRating_ratingValue']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparantly the average rating is higher when currency is not mentioned. That would make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 9.3.6:** Now we write a regular expression to extract emoticons from text.\n",
    "Start by locating all mouths ')' of emoticons, and develop the variations from there. Remember that paranthesis are special characters in regex, so you should use the escape character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Pattern: ([:;]-?[\\)\\(PpDOo\\*])|([\\)\\(\\*]-?[:;])\t Matched 21 patterns -----\n",
      "Match: :(\tContext:er galley :( at $9.40,\n",
      "Match: ):\tContext: of Minted):  if you o\n",
      "Match: :)\tContext:s Izzy!!! :) I'm a Squ\n",
      "Match: :-)\tContext:redit card:-) Very prof\n",
      "Match: :)\tContext:excellent :)\n",
      "The whole\n",
      "Match: :)\tContext: any one! :)\n",
      "Inflates \n",
      "Match: ):\tContext:RT version):\n",
      "\n",
      "* I call\n",
      "Match: :)\tContext:og treats :)\n",
      "I was the\n",
      "Match: :-)\tContext:day loans :-)\n",
      "first tim\n",
      "Match: :)\tContext: giveaway :)\n",
      "I had a c\n",
      "Match: :(\tContext:starving. :(\n",
      "I loved t\n",
      "Match: :(\tContext:hat book. :(\n",
      "My order \n",
      "Match: :)\tContext: anyone.  :)\n",
      "I traded \n",
      "Match: :-)\tContext:t service :-)\n",
      "booked a \n",
      "Match: :)\tContext:thank you :)\n",
      "A great e\n",
      "Match: :D\tContext: Wars MMO :D\n",
      "Like many\n",
      "Match: :D\tContext:ys again. :D\n",
      "From the \n",
      "Match: :)\tContext:orry free.:)\n",
      "I was loo\n",
      "Match: :(\tContext: cold now :(\n",
      "\n",
      "*Thanks*\n",
      "Match: :-(\tContext: a while. :-(\n",
      "Fees were\n",
      "Match: ;)\tContext:the title ;)\n",
      "\n",
      "<3 Jani\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#9.3.6\n",
    "sample_string = '\\n'.join(df[df['hasNumber']].sample(1000).reviewBody)\n",
    "explore_emoticon = e_re.ExploreRegex(sample_string)\n",
    "\n",
    "#emoticon_pattern matches all the most common emoticons:\n",
    "#    :), :(, :D, :o, :p, :* \n",
    "#also flipped, with noses '-', and winking eyes ';'.\n",
    "#also the heart emoticon '<3'\n",
    "\n",
    "emoticon_pattern = '([:;]-?[\\)\\(PpDOo\\*])|([\\)\\(\\*]-?[:;])|<3'\n",
    "explore_emoticon.explore_pattern(emoticon_pattern, n_samples = 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "nav_menu": {},
  "toc": {
   "nav_menu": {
    "height": "87px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
