
\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[left=3.00cm, right=3.00cm, top=3.00cm, bottom=3.00cm]{geometry}
\usepackage{personalpackage}
\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
	backgroundcolor=\color{backcolour},   
	commentstyle=\color{codegreen},
	keywordstyle=\color{magenta},
	numberstyle=\tiny\color{codegray},
	stringstyle=\color{codepurple},
	basicstyle=\ttfamily\footnotesize,
	breakatwhitespace=false,         
	breaklines=true,                 
	captionpos=b,                    
	keepspaces=true,                 
	numbers=left,                    
	numbersep=5pt,                  
	showspaces=false,                
	showstringspaces=false,
	showtabs=false,                  
	tabsize=4
}

\newcommand{\python}{\lstinline[language=Python]}

\lstset{style=mystyle}

\author{Sebastian Baltser}

\begin{document}
	\begin{enumerate}
		\item Date range is from 01-01-2007 to 21-08-2019. Words included in search was asylans√∏g, immigrant, flygtning, migrant, invandrer.
		\item So called webfeatures were not included as they did not follow the general structure of articles. There's 58 nan values in the contents data, which are removed
		\item Scraping articles containing words of interest took a total of 11 hours split on two computers. In total this adds up to x amount of calls.\\
		The files generated were concatenated using:
		
\begin{lstlisting}[language=Python]
def append_files(files):
	df = pd.concat([pd.read_csv(f, header = 0, sep = ';') for f in files], axis = 0)
	df.to_csv('./data/{}_concat.csv'.format(files[0].split('.')[0]), index = False) #Save to csv named as the first file with 'concat' appended

lst = ['dr_contents{}.csv', 'dr_links{}.csv', 'dr_log{}.csv']
for i in range(len(lst)):
	append_files([lst[i].format(''), lst[i].format('2')])
\end{lstlisting}
	\item stopord.txt is from:\\ "https://gist.github.com/berteltorp/0cf8a0c7afea7f25ed754f24cfc2467b?fbclid=IwAR2IMQuckiTXHfCwMYW4XH5c2wPVsNMgWM6goT4ZXtQqLoGGwS9Mo0Dyo5Q"
	Words manually added to stopwords: \\
	\python{['I', 'sige']}
	\end{enumerate}
	\subsection{TODO}
	\begin{enumerate}
		% https://api.statbank.dk/v1/data/VAN5/JSONSTAT?valuePresentation=Value&timeOrder=Ascending&STATSB=Verdensdele&ASYLTYPE=BRU&Tid=\%3E\%3D2007K1
		% https://api.statbank.dk/v1/data/VAN77/JSONSTAT?valuePresentation=Value&timeOrder=Ascending&OPHOLD=sum(1%2C2)&Tid=%3E%3D2007K1
	\end{enumerate}
	\subsection{Documentation}
		The process of scraping the DR homepage for articles are done within \lstinline[language=Python]|class dr_scraper|. This is done to simplify storing names of different files that are created. The class uses a couple of methods that are described below:
		\subsubsection{\python|def get_dr_article_links(self, searchterms, start, end)|}
			This function uses the search engine on dr.dk to search for terms in \python{searchterms}. When searching for a term a list of articles, where the term is present, is returned. The search term, including parameters, are included in the URL and the function accept parameters that can filter results. The URL also includes a page parameter but there is no 'total results' measure available so the function uses an infinite while loop that breaks when the return is empty. BeautifulSoup is used to parse the data returned from the URL and gathers the URL pointing to each article. The resulting data from the searchterm is then stored in a csv file.
			
			The function does this for every searchterm in the list \python {searchterms} and when done concatenates alle generated csv-files.
			
		\subsubsection{\python{def get_dr_article_contents(self, article_links)}}
			This function scrapes contents from the URLs provided in the list \python{article_links}. The data extracted for each article is title, publish date, and the text in the article. This data is coupled with the URLs for each article and returned as a pandas DataFrame.
			
		\subsubsection{\python{def batcher(self, article_links = None, batch_size = 100)}}
			This function splits the links into batches, that is scraped using \python{get_dr_article_contents} and saved into seperate csv-files. When done all batch-files are then merged to a single csv-file. This is done to ensure that the scraped data is not lost, should an error be raised during runtime.
			
			If a csv-file with contents is already in the cwd the new content is extended to this file. Also URLs in passed \python{article_links} already present in the contents file is removed from the list to ensure a URL is not scraped twice.
			
		\subsubsection{\python{def get_dr_article_count(self, start, end, project_name = 'Get article counts')}}
			Fetches publish date, title and url for all articles published for each day in the range defined by \python{start} and \python{end}. The function uses 'https://www.dr.dk/nyheder/allenyheder/' that lists all articles published on a particular date, appended to the URL. When done it saves the data to a csv-file.
\end{document}